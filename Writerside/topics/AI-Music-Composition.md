# AI Music Composition

<!--Writerside adds this topic when you create a new documentation project.
You can use it as a sandbox to play with Writerside features, and remove it from the TOC when you don't need it anymore.
If you want to re-add it for your experiments, click + to create a new topic, choose Topic from Template, and select the 
"Starter" template.-->

## Results

The speed of generating a simple music clip for AI is very fast, it requires only about one to two minutes to compose a 45-second music clip.

Example of Intro generated by AI

<video src="85bpmIntro.mp4"/>

Example of Climax generated by AI

<video src="85bpmClimax.mp4"/>

Example of Outro generated by AI

<video src="85bpmOutro.mp4"/>

Discussion: We can see that each separated clip is good result, but when they are combined together, they cannot match the style. Given that they are trained with the same clips of music sample, same statistics and same prompt with the same AI model. The difference is just they get sample from different part of same clips of music. Therefore, it still requires a long training or arrangement process.

However, it is hard for AI to get sample from the whole song but not the separate part. Here is one of the output that is not optimal:

<video src="Notgoodexample.mp4"/>

We can see that if we feed the whole music clip to the model, the music generated don't suit to our prompt given, we input canton pop and piano, but this clip shows not only the piano sound, but with other instruments. And the tone is strange too, we can't classify if it is an intro, climax or outro. Therefore, feeding the whole music clip to AI model will lead to a lower quality of music. We should compose each part separately in this case.

## Using OpenAI JukeBox

First, for defining parameters, we have to decide on the mood, genre and style of the music that we would like to generate, for example, we could choose a Canton pop music, with piano and separate it to three parts: Intro, Climax and Outro, generating three output Next, we need to select a model. There are many pre-trained models which are suitable for music generation, like MIDI-based mode or waveform generation model etc. OpenAI JukeBox is a model that trained on a dataset of 1.2 million songs spanning various genres and languages, along with their corresponding lyrics and metadata.

We need to input data to train the mode in order to get ideal results. In this case, we can input 20 canton pop samples which last for about 20 seconds (due to the limitations of the model). We have to set the number of sampling levels, in this case, the sampling level is set to level one due to hardware limitation of hardware, and the rate of 85 bpm is selected. Giving the prompt: Canton Pop, Piano, Intro/Climax/Outro. Finally, three parts of music is composited out.


In the current stage, we still need to use a lot of prompts which is no natural language to give command to the AI model to generate a music clip. This is an example of JukeBox prompt.

```Shell
mpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_prior,all_fp16,cpu_ema --name=small_prior \
--sample_length=2097152 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \
--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=1 --weight_decay=0.01 --save_iters=1000
```

### Usage explaination
`mpiexec -n {ngpus} python jukebox/train.py`:  This part of the command is using MPI (Message Passing Interface) to run the training script on multiple GPUs. The number of GPUs is specified by the variable `ngpus`.

### Specified options

--hps=small_vqvae,small_prior,all_fp16,cpu_ema
: Specifies the hyperparameters for training. It includes configurations for the small VQ-VAE, small prior, mixed-precision training (all_fp16), and CPU-based exponential moving average (cpu_ema).

--name=small_prior
: Sets the name of the training run to "small_prior."

--sample_length=2097152
: Sets the length of the training samples. In this case, the samples are 2,097,152 audio samples long.

--bs=4
: Sets the batch size for training. The batch size is 4.

--audio_files_dir=[audio_files_dir]
: Specifies the directory containing the audio files for training. The actual directory path is expected to be provided for `audio_files_dir`.

--labels=False
: Indicates whether the training data includes labels. In this case, it's set to False, suggesting that the training data does not have associated labels.

--train --test
: Specifies that the script should perform both training and testing. Training is to update the model parameters, while testing is often used to evaluate the model's performance on a validation set.

--aug_shift --aug_blend
: Enables data augmentation by shifting and blending the audio during training.

--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar
: Specifies the path to restore the VQ-VAE model from a checkpoint. The prior model often relies on the features learned by the VQ-VAE.

--prior --levels=2 --level=1
: Indicates that the training is for the prior model (--prior), and the prior model is at level=1 out of levels=2. The prior model typically involves generating coarse features.

--weight_decay=0.01
: Sets the weight decay, which is a regularization term applied during training to prevent overfitting.

--save_iters=1000
: Specifies the interval at which the model checkpoints will be saved. In this case, a checkpoint will be saved every 1000 iterations.

Discussion: Writing prompts to AI model is still a difficult job for the general public. Only people with professional knowledge in coding can understand and write this code easily.